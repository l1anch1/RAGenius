# RAG System Evaluation - ä½¿ç”¨æŒ‡å—

## âš ï¸ é‡è¦å‰æ

**Ragas éœ€è¦ OpenAI API Key** æ¥è¯„ä¼°ç­”æ¡ˆè´¨é‡ã€‚è¯·ç¡®ä¿ï¼š

```bash
# æ–¹å¼ 1: åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®
OPENAI_API_KEY=sk-your-openai-key-here

# æ–¹å¼ 2: ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY=sk-your-key-here

# æ–¹å¼ 3: ä½¿ç”¨ç°æœ‰çš„ RAGenius key
export OPENAI_API_KEY=$(grep LLM_OPENAI_API_KEY .env | cut -d '=' -f2)
```

ğŸ’¡ **ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ** Ragas ä½¿ç”¨ LLM ä½œä¸º"è¯„åˆ¤è€…"æ¥è¯„ä¼°ç­”æ¡ˆçš„å¿ å®åº¦å’Œç›¸å…³æ€§ã€‚

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### Step 1: å¯åŠ¨åç«¯æœåŠ¡

```bash
# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd /path/to/RAGenius

# å¯åŠ¨ Docker æœåŠ¡
docker compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆçº¦ 30 ç§’ï¼‰
docker compose logs -f backend

# çœ‹åˆ° "Running on http://0.0.0.0:8000" è¡¨ç¤ºå¯åŠ¨æˆåŠŸ
# æŒ‰ Ctrl+C é€€å‡ºæ—¥å¿—æŸ¥çœ‹
```

### Step 2: é…ç½® API Keyï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰

```bash
# æ·»åŠ åˆ° .env æ–‡ä»¶
echo "OPENAI_API_KEY=sk-your-key-here" >> .env
```

### Step 3: æµ‹è¯•åç«¯è¿æ¥

```bash
# æµ‹è¯•è¿æ¥
python3 evaluation/test_connection.py

# åº”è¯¥çœ‹åˆ°ï¼š
# âœ… Health check passed
# âœ… System info retrieved
# âœ… Documents retrieved
# âœ… Query successful
```

### Step 4: è¿è¡Œè¯„ä¼°

```bash
# æ–¹å¼ A: ä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰
./evaluation/run_evaluation.sh

# æ–¹å¼ B: ç›´æ¥è¿è¡Œ Python
python3 evaluation/scripts/evaluate_rag.py
```

## ğŸ“‹ è¯¦ç»†æµç¨‹

### 1. å®Œæ•´è¯„ä¼°ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

```bash
./evaluation/run_evaluation.sh
```

**é¢„è®¡æ—¶é—´**ï¼š5-10 åˆ†é’Ÿï¼ˆå–å†³äº API é€Ÿåº¦ï¼‰

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `evaluation/results/EVALUATION_REPORT.md` - è¯¦ç»†æŠ¥å‘Š
- `evaluation/results/evaluation_results.svg` - å¯è§†åŒ–å›¾è¡¨
- `evaluation/results/evaluation_report.json` - æœºå™¨å¯è¯»ç»“æœ

### 2. å¿«é€Ÿè¯„ä¼°ï¼ˆä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“ï¼‰

```bash
# å¦‚æœå·²ç»ä¸Šä¼ è¿‡æ–‡æ¡£ï¼Œå¯ä»¥è·³è¿‡ä¸Šä¼ æ­¥éª¤
./evaluation/run_evaluation.sh --skip-upload
```

**é¢„è®¡æ—¶é—´**ï¼š3-5 åˆ†é’Ÿ

### 3. è¯„ä¼°è¿œç¨‹éƒ¨ç½²

```bash
# è¯„ä¼°éƒ¨ç½²åœ¨æœåŠ¡å™¨ä¸Šçš„ç³»ç»Ÿ
./evaluation/run_evaluation.sh --backend-url http://your-server.com:8000
```

### 4. è‡ªå®šä¹‰è¯„ä¼°

```python
# åˆ›å»ºè‡ªå·±çš„è¯„ä¼°è„šæœ¬
from pathlib import Path
from evaluation.scripts.evaluate_rag import RAGEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = RAGEvaluator(
    test_data_path="evaluation/data/test_dataset.json",
    output_dir="evaluation/results",
    backend_url="http://localhost:8000"
)

# è¿è¡Œè¯„ä¼°
evaluator.run_full_evaluation(skip_upload=False)
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æµ‹è¯•æ•°æ®é›†

ç¼–è¾‘ `evaluation/data/test_dataset.json`:

```json
{
  "test_cases": [
    {
      "question": "ä½ çš„é—®é¢˜ï¼Ÿ",
      "ground_truth": "æ ‡å‡†ç­”æ¡ˆ",
      "context_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
    }
  ]
}
```

### ä½¿ç”¨è‡ªå·±çš„çŸ¥è¯†åº“

```python
# 1. ä¸Šä¼ ä½ çš„æ–‡æ¡£
evaluator.upload_knowledge_base(Path("/path/to/your/docs"))

# 2. é‡å»ºçŸ¥è¯†åº“
evaluator.rebuild_knowledge_base()

# 3. è¿è¡Œè¯„ä¼°
dataset = evaluator.prepare_evaluation_dataset()
results = evaluator.evaluate_with_ragas(dataset)
```

### è°ƒæ•´è¯„ä¼°æŒ‡æ ‡

ç¼–è¾‘ `evaluation/scripts/evaluate_rag.py`:

```python
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_similarity,      # æ·»åŠ æ–°æŒ‡æ ‡
    answer_correctness,     # æ·»åŠ æ–°æŒ‡æ ‡
)

# åœ¨ evaluate_with_ragas å‡½æ•°ä¸­
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_similarity,   # ä½¿ç”¨æ–°æŒ‡æ ‡
        answer_correctness,
    ],
)
```

## ğŸ“Š ç»“æœè§£è¯»

### æŒ‡æ ‡å«ä¹‰

| æŒ‡æ ‡ | è¯´æ˜ | è‰¯å¥½é˜ˆå€¼ |
|------|------|---------|
| **Faithfulness** | ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢çš„ä¸Šä¸‹æ–‡ | â‰¥ 85% |
| **Answer Relevancy** | ç­”æ¡ˆæ˜¯å¦å›ç­”äº†é—®é¢˜ | â‰¥ 80% |
| **Context Precision** | æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ | â‰¥ 75% |
| **Context Recall** | æ˜¯å¦æ£€ç´¢åˆ°æ‰€æœ‰å¿…è¦ä¿¡æ¯ | â‰¥ 80% |

### æ€§èƒ½ç­‰çº§

- **ğŸŸ¢ Excellent** (â‰¥85%): ç”Ÿäº§å°±ç»ª
- **ğŸŸ¡ Good** (75-84%): å¯ç”¨ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´
- **ğŸ”´ Needs Improvement** (<75%): éœ€è¦ä¼˜åŒ–

### ä¼˜åŒ–å»ºè®®

**å¦‚æœ Faithfulness ä½**:
- ä¼˜åŒ– promptï¼Œå¼ºè°ƒä½¿ç”¨ä¸Šä¸‹æ–‡
- é™ä½ temperature (0.0-0.2)
- æ£€æŸ¥æ–‡æ¡£è´¨é‡

**å¦‚æœ Answer Relevancy ä½**:
- æ”¹è¿› prompt å¼•å¯¼
- ä½¿ç”¨ few-shot examples
- ä¼˜åŒ–ç­”æ¡ˆæ ¼å¼

**å¦‚æœ Context Precision ä½**:
- è°ƒæ•´ chunk size
- å¯ç”¨/ä¼˜åŒ– reranking
- å¢åŠ  score threshold

**å¦‚æœ Context Recall ä½**:
- å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°é‡
- ä¼˜åŒ– query expansion
- æ£€æŸ¥åµŒå…¥æ¨¡å‹è´¨é‡

## ğŸš€ æŒç»­è¯„ä¼°

### è®¾ç½®å®šæœŸè¯„ä¼°

```bash
# åˆ›å»º cron ä»»åŠ¡
crontab -e

# æ¯å¤©å‡Œæ™¨ 2 ç‚¹è¿è¡Œè¯„ä¼°
0 2 * * * cd /path/to/RAGenius && ./evaluation/run_evaluation.sh --skip-upload
```

### CI/CD é›†æˆ

```yaml
# .github/workflows/evaluation.yml
name: RAG Evaluation

on:
  schedule:
    - cron: '0 0 * * 0'  # æ¯å‘¨è¿è¡Œ

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Start backend
        run: docker compose up -d
      - name: Run evaluation
        run: |
          pip install -r evaluation/requirements.txt
          ./evaluation/run_evaluation.sh --skip-upload
      - name: Upload results
        uses: actions/upload-artifact@v2
        with:
          name: evaluation-results
          path: evaluation/results/
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é¦–æ¬¡è¯„ä¼°**: ä½¿ç”¨å®Œæ•´æµç¨‹ï¼Œä¸Šä¼ ç¤ºä¾‹æ–‡æ¡£
2. **æ—¥å¸¸è¯„ä¼°**: ä½¿ç”¨ `--skip-upload` å¿«é€Ÿè¯„ä¼°
3. **ç”Ÿäº§è¯„ä¼°**: è¯„ä¼°å‰å¤‡ä»½çŸ¥è¯†åº“
4. **å¯¹æ¯”è¯„ä¼°**: ä¿å­˜æ¯æ¬¡ç»“æœï¼Œè¿½è¸ªè¶‹åŠ¿
5. **A/B æµ‹è¯•**: ä¿®æ”¹é…ç½®åé‡æ–°è¯„ä¼°ï¼Œå¯¹æ¯”ç»“æœ

## ğŸ“š ç›¸å…³èµ„æº

- [Ragas æ–‡æ¡£](https://docs.ragas.io/)
- [è¯„ä¼°å®Œæ•´æŠ¥å‘Š](./results/EVALUATION_REPORT.md)
- [æµ‹è¯•æ•°æ®é›†](./data/test_dataset.json)
- [ä¸» README](../README.md)

---

**æœ‰é—®é¢˜ï¼Ÿ** æŸ¥çœ‹ [FAQ](./README.md#-å¸¸è§é—®é¢˜) æˆ–æ Issue

