# RAG 系统基础知识

## 什么是 RAG？

RAG（Retrieval Augmented Generation，检索增强生成）是一种结合信息检索和文本生成的先进技术。它的核心思想是在生成答案之前，先从知识库中检索相关信息，然后将这些信息作为上下文提供给大语言模型（LLM），从而生成更准确、更有依据的回答。

RAG 的主要优势包括：

1. **减少幻觉**：通过提供真实的文档上下文，避免模型凭空捏造信息
2. **知识更新**：无需重新训练模型即可更新知识库
3. **可追溯性**：每个答案都有明确的来源，便于验证
4. **领域适应**：可以快速适应特定领域的知识需求

## RAG 系统的核心组件

一个完整的 RAG 系统通常包含以下核心组件：

### 1. 文档处理模块

负责处理原始文档，主要功能包括：
- **文档加载**：支持多种格式（PDF、Word、Markdown、CSV 等）
- **文档分块（Chunking）**：将长文档切分成较小的片段
- **向量化（Embedding）**：将文本转换为数值向量

文档分块是非常重要的环节。合适的分块策略需要考虑：
- **块大小（Chunk Size）**：通常在 200-1000 tokens 之间
- **重叠（Overlap）**：相邻块之间的重叠部分，保证语义完整性
- **分块策略**：按段落、按句子或按固定长度

### 2. 向量数据库

向量数据库负责存储文档的向量表示，并提供高效的相似度搜索。常见的向量数据库包括：
- **ChromaDB**：轻量级，适合开发和中小型应用
- **Pinecone**：云端托管，高性能
- **Weaviate**：开源，功能丰富
- **Milvus**：适合大规模部署

向量数据库的核心功能是 **相似度搜索**：给定查询向量，快速找出最相似的 k 个文档向量。

### 3. 嵌入模型（Embedding Model）

嵌入模型将文本转换为固定维度的向量。这些向量能够捕捉文本的语义信息，使得语义相似的文本在向量空间中距离较近。

常用的嵌入模型：
- **OpenAI Embeddings**：高质量，但需要 API 调用
- **sentence-transformers**：开源，可本地部署
- **bge 系列**：针对中文优化
- **e5 系列**：多语言支持

### 4. 检索模块

检索模块负责根据用户查询找到最相关的文档片段。主要方法包括：

#### 向量检索（Semantic Search）
使用向量相似度进行语义搜索。常见的相似度计算方法：
- **余弦相似度（Cosine Similarity）**：最常用，关注向量方向
- **点积（Dot Product）**：考虑向量长度
- **欧氏距离（Euclidean Distance）**：直线距离

#### 关键词检索
基于词频和倒排索引的传统检索方法，如 **BM25 算法**。BM25 擅长精确的关键词匹配。

#### 混合检索（Hybrid Retrieval）
结合向量检索和关键词检索的优势。通常使用融合算法（如 RRF - Reciprocal Rank Fusion）整合两种方法的结果，既能理解语义又不会错过重要的关键词。

### 5. 重排序（Reranking）

初步检索后，使用更精确但速度较慢的模型对结果进行重新排序。

**Cross-Encoder** 是常用的重排序模型：
- 同时处理查询和文档
- 计算更精确的相关性分数
- 虽然速度慢，但准确度高

典型流程：
1. 快速的向量检索获得候选集（如 Top 50）
2. Cross-Encoder 重排序选出最相关的文档（如 Top 10）
3. 将精选文档提供给 LLM

### 6. 生成模块

将检索到的上下文和用户查询组合成 prompt，输入到大语言模型生成最终答案。

关键要素：
- **Prompt Engineering**：设计合适的提示词
- **上下文管理**：处理上下文长度限制
- **输出控制**：格式化、引用来源等

## RAG 的高级技术

### 查询扩展（Query Expansion）

将用户的原始查询改写或扩展为多个相关查询：
- 使用 LLM 生成不同表述
- 添加同义词
- 从不同角度重新表达

查询扩展能提高检索召回率，因为同一信息可能在文档中以不同方式表达。

### MMR 算法（Maximal Marginal Relevance）

MMR 算法平衡检索结果的相关性和多样性：
- 不仅考虑文档与查询的相似度
- 降低与已选文档高度相似的文档排名
- 通过 lambda 参数控制权重：
  - lambda=1：完全关注相关性
  - lambda=0：完全关注多样性

### 语义缓存（Semantic Cache）

缓存相似查询的结果：
- 使用向量相似度判断查询是否相似
- 相似度超过阈值时直接返回缓存结果
- 显著降低成本和延迟

### Agent-based RAG

让 AI Agent 自主决定何时、如何使用 RAG：
- 判断是否需要检索
- 决定检索的深度和广度
- 迭代优化检索策略
- 整合多个知识源

通过 ReAct、Tool Use 等技术实现。

## RAG 系统的评估

### 检索评估指标
- **Precision**（准确率）：检索结果中相关文档的比例
- **Recall**（召回率）：相关文档被检索到的比例
- **NDCG**：考虑排序的评估指标
- **MRR**：第一个相关结果的排名

### 生成评估指标
- **BLEU**：基于 n-gram 重叠
- **ROUGE**：召回导向
- **BERTScore**：基于语义相似度

### RAG 专用指标
- **Faithfulness**（忠实度）：答案是否基于提供的上下文
- **Answer Relevancy**（答案相关性）：答案是否回答了问题
- **Context Precision**：检索结果中相关文档的比例
- **Context Recall**：相关文档被检索到的比例

**Ragas** 框架提供了这些专门的 RAG 评估工具。

## RAG 系统的优化策略

### 1. 选择合适的嵌入模型
- 考虑语言（中文、英文、多语言）
- 评估性能和成本
- 针对领域进行微调

### 2. 优化分块策略
- 实验不同的 chunk size
- 调整 overlap 比例
- 使用语义分块

### 3. 实现混合检索
- 结合向量检索和 BM25
- 使用融合算法（RRF）
- 根据场景调整权重

### 4. 应用重排序
- 使用 Cross-Encoder
- 平衡准确度和速度
- 考虑两阶段检索

### 5. 实现缓存机制
- 精确匹配缓存
- 语义缓存
- 设置合理的过期时间

### 6. 优化向量数据库
- 选择合适的索引方法（HNSW、IVF）
- 调整搜索参数
- 考虑分片和副本

### 7. Prompt Engineering
- 明确指示使用上下文
- 控制输出格式
- 处理边界情况

### 8. 持续评估和迭代
- A/B 测试
- 收集用户反馈
- 监控关键指标

## 多语言支持

处理多语言文档的策略：
- 使用多语言嵌入模型（multilingual-e5、LaBSE）
- 实现语言检测
- 使用多语言 LLM
- 建立语言特定的索引

## 实时数据更新

处理实时数据的方法：
- **增量索引**：只对新增/修改文档生成向量
- **版本控制**：标记文档版本和时间戳
- **定期重建**：根据更新频率安排
- **缓存失效**：文档更新时清除缓存

## Few-shot Learning

在 prompt 中提供示例来引导模型：
- 包含「问题-上下文-答案」示例
- 帮助模型理解输出格式
- 适合特定风格或领域
- 相比 zero-shot 更稳定

## 总结

RAG 是一种强大的技术，能够让大语言模型访问外部知识，生成更准确、可追溯的答案。构建高质量的 RAG 系统需要在文档处理、检索、生成等各个环节进行优化，并通过持续评估和迭代来提升性能。



