# Device settings
DEVICE=cpu

# Data directories
DATA_DIR="./data"
DOCUMENTS_DIR="./data/documents"
VECTOR_DB_DIR="./data/vectordb"

# LLM model settings
LLM_USE_OPENAI=true  # Set to 'true' if using OpenAI API, or 'false' otherwise
LLM_OPENAI_API_KEY="YOUR_OPENAI_API_KEY"  # Replace with your actual OpenAI API key
LLM_OPENAI_MODEL="gpt-4o"  # Default model to use with OpenAI
LLM_OPENAI_API_BASE="YOUR_OPENAI_API_BASE"  # Base URL for OpenAI API (if different)
LLM_LOCAL_MODEL="deepseek-r1:14b"  # Default model name for local use
LLM_TEMPERATURE=0.1  # Temperature for randomness in responses
LLM_TOP_P=0.6  # Nucleus sampling parameter
LLM_NUM_CTX=8192  # Number of context tokens for the model
LLM_NUM_PREDICT=2048  # Number of tokens to predict
LLM_NUM_THREAD=12  # Number of threads to use

# Embedding settings
EMBEDDING_MODEL="BAAI/bge-base-zh-v1.5"  # Default embedding model

# Chunking settings
CHUNK_SIZE=600  # Size of chunks for processing
CHUNK_OVERLAP=150  # Overlap size between chunks

# Retrieval settings
SEARCH_K=8  # Number of documents to return during retrieval
# Ollama settings (for local models)
OLLAMA_BASE_URL="http://localhost:11434"  # Use http://host.docker.internal:11434 when running in Docker
