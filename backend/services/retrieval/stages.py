"""
Retrieval Pipeline Stages
æ£€ç´¢æµæ°´çº¿å„é˜¶æ®µå®šä¹‰

æ¯ä¸ª Stage æ˜¯ç‹¬ç«‹çš„ã€å¯æ’æ‹”çš„å¤„ç†å•å…ƒï¼Œéµå¾ªç»Ÿä¸€æ¥å£ã€‚
"""
import os
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from langchain_core.documents import Document

logger = logging.getLogger(__name__)


# =============================================================================
# æ•°æ®ç»“æ„
# =============================================================================

@dataclass
class ScoredDocument:
    """å¸¦åˆ†æ•°çš„æ–‡æ¡£"""
    document: Document
    score: float
    source: str = "unknown"
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def page_content(self) -> str:
        return self.document.page_content
    
    @property
    def doc_metadata(self) -> Dict[str, Any]:
        return self.document.metadata


@dataclass
class RetrievalContext:
    """
    æ£€ç´¢ä¸Šä¸‹æ–‡ - åœ¨å„é˜¶æ®µä¹‹é—´ä¼ é€’çš„æ•°æ®å®¹å™¨
    
    æ¯ä¸ªé˜¶æ®µè¯»å–éœ€è¦çš„æ•°æ®ï¼Œå†™å…¥äº§å‡ºçš„æ•°æ®ï¼Œ
    å½¢æˆæ•°æ®æµï¼šquery â†’ expanded_queries â†’ retrieved_docs â†’ fused_docs â†’ ...
    """
    # è¾“å…¥
    original_query: str
    
    # å„é˜¶æ®µäº§å‡º
    expanded_queries: List[str] = field(default_factory=list)
    retrieved_results: Dict[str, Dict[str, List[ScoredDocument]]] = field(default_factory=dict)
    fused_documents: List[ScoredDocument] = field(default_factory=list)
    reranked_documents: List[ScoredDocument] = field(default_factory=list)
    truncated_documents: List[ScoredDocument] = field(default_factory=list)  # æ™ºèƒ½æˆªæ–­å
    final_documents: List[ScoredDocument] = field(default_factory=list)
    
    # ç½®ä¿¡åº¦æ ‡è®°
    low_confidence: bool = False  # æ˜¯å¦ä½ç½®ä¿¡åº¦ï¼ˆæ²¡æœ‰é«˜ç›¸å…³æ–‡æ¡£ï¼‰
    
    # å…ƒæ•°æ®ï¼ˆå„é˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯ï¼‰
    stage_metadata: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    def to_langchain_documents(self) -> List[Document]:
        """è½¬æ¢ä¸º LangChain Document åˆ—è¡¨"""
        return [sd.document for sd in self.final_documents]


# =============================================================================
# Stage åŸºç±»
# =============================================================================

class RetrievalStage(ABC):
    """
    æ£€ç´¢é˜¶æ®µåŸºç±»
    
    æ‰€æœ‰é˜¶æ®µå¿…é¡»å®ç°:
    - name: é˜¶æ®µåç§°
    - execute(): æ‰§è¡Œé€»è¾‘
    - is_enabled(): æ˜¯å¦å¯ç”¨
    """
    
    @property
    @abstractmethod
    def name(self) -> str:
        """é˜¶æ®µåç§°"""
        pass
    
    @abstractmethod
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        """
        æ‰§è¡Œé˜¶æ®µé€»è¾‘
        
        Args:
            context: æ£€ç´¢ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å‰åºé˜¶æ®µçš„äº§å‡ºï¼‰
        
        Returns:
            æ›´æ–°åçš„æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        pass
    
    @abstractmethod
    def is_enabled(self) -> bool:
        """è¯¥é˜¶æ®µæ˜¯å¦å¯ç”¨"""
        pass
    
    def get_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®ï¼ˆç”¨äº API å±•ç¤ºï¼‰"""
        return {}
    
    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        pass


# =============================================================================
# Stage å®ç°
# =============================================================================

class QueryExpansionStage(RetrievalStage):
    """
    æŸ¥è¯¢æ‰©å±•é˜¶æ®µ
    
    ä½¿ç”¨ QueryExpansionLLMManager ç®¡ç†çš„è½»é‡ LLMï¼ˆå¦‚ gpt-4o-miniï¼‰
    ç”Ÿæˆå¤šè§’åº¦æŸ¥è¯¢ï¼Œæé«˜å¬å›ç‡ã€‚
    
    æ³¨æ„ï¼šå³ä½¿ enabled=Falseï¼Œé˜¶æ®µä»ç„¶æ‰§è¡Œï¼Œåªæ˜¯è¿”å›åŸå§‹æŸ¥è¯¢ã€‚
    è¿™ç¡®ä¿ expanded_queries å§‹ç»ˆæœ‰å€¼ã€‚
    """
    
    def __init__(self):
        from config import (
            QUERY_EXPANSION_ENABLED,
            QUERY_EXPANSION_N_SUBQUERIES,
            QUERY_EXPANSION_INCLUDE_ORIGINAL
        )
        
        self._llm_manager = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.enabled = QUERY_EXPANSION_ENABLED
        self.n_subqueries = QUERY_EXPANSION_N_SUBQUERIES
        self.include_original = QUERY_EXPANSION_INCLUDE_ORIGINAL
        
        from config import QUERY_EXPANSION_PROMPT_TEMPLATE
        self._prompt_template = QUERY_EXPANSION_PROMPT_TEMPLATE
    
    @property
    def name(self) -> str:
        return "Query Expansion"
    
    def is_enabled(self) -> bool:
        # å§‹ç»ˆè¿”å› Trueï¼Œç¡®ä¿ expanded_queries è¢«è®¾ç½®
        # å®é™…æ‰©å±•é€»è¾‘åœ¨ execute ä¸­æ ¹æ® self.enabled åˆ¤æ–­
        return True
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        
        logger.info(f"[QueryExpansion] Input: \"{context.original_query}\"")
        
        @timed("Query Expansion")
        def _do_expand():
            if not self.enabled:
                logger.info("[QueryExpansion] â­ Disabled, using original query only")
                return [context.original_query]
            
            queries = []
            if self.include_original:
                queries.append(context.original_query)
            
            try:
                llm = self._get_llm()
                if llm is None:
                    return [context.original_query]
                
                prompt = self._prompt_template.format(
                    n=self.n_subqueries,
                    query=context.original_query
                )
                
                response = llm.invoke(prompt)
                content = response.content if hasattr(response, 'content') else str(response)
                
                new_queries = [q.strip() for q in content.strip().split('\n') if q.strip()]
                for q in new_queries[:self.n_subqueries]:
                    if q and q not in queries:
                        queries.append(q)
                
            except Exception as e:
                logger.error(f"Query expansion failed: {e}")
                if context.original_query not in queries:
                    queries.append(context.original_query)
            
            return queries
        
        context.expanded_queries = _do_expand()
        
        # æ‰“å°æ‰©å±•çš„æŸ¥è¯¢
        logger.info(f"[QueryExpansion] Output: {len(context.expanded_queries)} queries")
        for i, q in enumerate(context.expanded_queries, 1):
            logger.info(f"    {i}. {q}")
        
        context.stage_metadata["query_expansion"] = {
            "n_queries": len(context.expanded_queries),
            "queries": context.expanded_queries
        }
        return context
    
    def _get_llm(self) -> Any:
        """é€šè¿‡ Manager è·å– LLMï¼ˆç»Ÿä¸€ç®¡ç†ã€å¸¦ç¼“å­˜ï¼‰"""
        if self._llm_manager is None:
            from managers import QueryExpansionLLMManager
            self._llm_manager = QueryExpansionLLMManager()
        return self._llm_manager.get_llm()
    
    def get_config(self) -> Dict[str, Any]:
        from config import QUERY_EXPANSION_MODEL
        return {
            "enabled": self.enabled,
            "n_subqueries": self.n_subqueries,
            "model": QUERY_EXPANSION_MODEL
        }
    
    def update_config(self, **kwargs):
        if 'enabled' in kwargs:
            self.enabled = kwargs['enabled']
        if 'n_subqueries' in kwargs:
            self.n_subqueries = kwargs['n_subqueries']


class HybridRetrievalStage(RetrievalStage):
    """æ··åˆæ£€ç´¢é˜¶æ®µ (Embedding + BM25)"""
    
    def __init__(self, vector_store: Any = None):
        from config import HYBRID_TOP_K_PER_QUERY
        from concurrent.futures import ThreadPoolExecutor
        
        self._vector_store = vector_store
        self._bm25_retriever = None
        self._documents_hash = None
        
        self.top_k_per_query = HYBRID_TOP_K_PER_QUERY
        
        self._query_executor = ThreadPoolExecutor(max_workers=8)
        self._retrieval_executor = ThreadPoolExecutor(max_workers=4)
    
    @property
    def name(self) -> str:
        return "Hybrid Retrieval"
    
    def is_enabled(self) -> bool:
        return True  # æ£€ç´¢é˜¶æ®µå§‹ç»ˆå¯ç”¨
    
    def set_vector_store(self, vector_store: Any):
        self._vector_store = vector_store
        self._rebuild_bm25_index()
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        
        @timed("Hybrid Retrieval")
        def _do_retrieve():
            queries = context.expanded_queries or [context.original_query]
            
            futures = {}
            for query in queries:
                futures[query] = self._query_executor.submit(self._retrieve_single, query)
            
            all_results = {}
            for query, future in futures.items():
                try:
                    all_results[query] = future.result(timeout=60)
                except Exception as e:
                    logger.error(f"Retrieval failed for '{query}': {e}")
                    all_results[query] = {"embedding": [], "bm25": []}
            
            return all_results
        
        context.retrieved_results = _do_retrieve()
        
        total_embedding = sum(len(r.get("embedding", [])) for r in context.retrieved_results.values())
        total_bm25 = sum(len(r.get("bm25", [])) for r in context.retrieved_results.values())
        
        # æ‰“å°æ£€ç´¢ç»“æœæ‘˜è¦
        logger.info(f"[HybridRetrieval] Output: {total_embedding} embedding + {total_bm25} BM25")
        for query, results in context.retrieved_results.items():
            emb_count = len(results.get("embedding", []))
            bm25_count = len(results.get("bm25", []))
            short_query = query[:35] + "..." if len(query) > 35 else query
            logger.info(f"    â†’ \"{short_query}\": emb={emb_count}, bm25={bm25_count}")
        
        context.stage_metadata["hybrid_retrieval"] = {
            "total_embedding_results": total_embedding,
            "total_bm25_results": total_bm25
        }
        return context
    
    def _retrieve_single(self, query: str) -> Dict[str, List[ScoredDocument]]:
        """å•ä¸ªæŸ¥è¯¢çš„æ··åˆæ£€ç´¢"""
        results = {}
        try:
            embedding_future = self._retrieval_executor.submit(self._embedding_retrieve, query)
            bm25_future = self._retrieval_executor.submit(self._bm25_retrieve, query)
            
            results["embedding"] = embedding_future.result(timeout=30)
            results["bm25"] = bm25_future.result(timeout=30)
        except Exception as e:
            logger.error(f"Parallel retrieval failed: {e}")
            results["embedding"] = self._embedding_retrieve(query)
            results["bm25"] = self._bm25_retrieve(query)
        return results
    
    def _embedding_retrieve(self, query: str) -> List[ScoredDocument]:
        if self._vector_store is None:
            return []
        try:
            results = self._vector_store.similarity_search_with_score(query, k=self.top_k_per_query)
            return [
                ScoredDocument(document=doc, score=1/(1+score), source="embedding")
                for doc, score in results
            ]
        except Exception as e:
            logger.error(f"Embedding retrieval failed: {e}")
            return []
    
    def _bm25_retrieve(self, query: str) -> List[ScoredDocument]:
        if self._bm25_retriever is None:
            self._rebuild_bm25_index()
        if self._bm25_retriever is None:
            return []
        
        # BM25Retriever è¿”å› Dict åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸º ScoredDocument
        results = self._bm25_retriever.retrieve(query, self.top_k_per_query)
        return [
            ScoredDocument(document=r["document"], score=r["score"], source="bm25")
            for r in results
        ]
    
    def _rebuild_bm25_index(self):
        """é‡å»º BM25 ç´¢å¼•"""
        if self._vector_store is None:
            return
        try:
            from .bm25 import BM25Retriever
            
            collection = self._vector_store._collection
            results = collection.get(include=["documents", "metadatas"])
            
            if not results or not results.get("documents"):
                return
            
            documents = []
            for i, doc_content in enumerate(results["documents"]):
                metadata = results["metadatas"][i] if results.get("metadatas") else {}
                documents.append(Document(page_content=doc_content, metadata=metadata))
            
            new_hash = hash(tuple(d.page_content[:100] for d in documents[:100]))
            if new_hash == self._documents_hash:
                return
            
            self._documents_hash = new_hash
            self._bm25_retriever = BM25Retriever(documents)
            
        except Exception as e:
            logger.error(f"Failed to rebuild BM25 index: {e}")
    
    def get_config(self) -> Dict[str, Any]:
        return {"top_k_per_query": self.top_k_per_query}
    
    def update_config(self, **kwargs):
        if 'top_k_per_query' in kwargs:
            self.top_k_per_query = kwargs['top_k_per_query']


class RRFFusionStage(RetrievalStage):
    """RRF èåˆé˜¶æ®µ"""
    
    def __init__(self):
        from config import RRF_K, RRF_TOP_K
        
        self.k = RRF_K
        self.top_k = RRF_TOP_K
    
    @property
    def name(self) -> str:
        return "RRF Fusion"
    
    def is_enabled(self) -> bool:
        return True
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        from collections import defaultdict
        
        @timed("RRF Fusion")
        def _do_fuse():
            # æ‰å¹³åŒ–ç»“æœ
            flattened = {}
            for query, strategy_results in context.retrieved_results.items():
                for strategy, docs in strategy_results.items():
                    key = f"{query[:30]}_{strategy}"
                    flattened[key] = docs
            
            # RRF è®¡ç®—
            doc_scores = defaultdict(lambda: {"score": 0.0, "doc": None, "sources": []})
            
            for source_name, doc_list in flattened.items():
                for rank, scored_doc in enumerate(doc_list, start=1):
                    doc_key = (scored_doc.page_content[:400], scored_doc.doc_metadata.get('source', ''))
                    rrf_score = 1.0 / (self.k + rank)
                    
                    doc_scores[doc_key]["score"] += rrf_score
                    doc_scores[doc_key]["sources"].append(source_name)
                    if doc_scores[doc_key]["doc"] is None:
                        doc_scores[doc_key]["doc"] = scored_doc.document
            
            # æ’åº
            sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1]["score"], reverse=True)
            
            return [
                ScoredDocument(
                    document=doc_info["doc"],
                    score=doc_info["score"],
                    source="rrf_fusion",
                    metadata={"original_sources": doc_info["sources"]}
                )
                for _, doc_info in sorted_docs[:self.top_k]
            ]
        
        context.fused_documents = _do_fuse()
        
        # æ‰“å° RRF èåˆç»“æœ
        logger.info(f"[RRFFusion] Output: {len(context.fused_documents)} documents (top 5 shown)")
        for i, doc in enumerate(context.fused_documents[:5], 1):
            source = os.path.basename(doc.doc_metadata.get('source', 'unknown'))
            preview = doc.page_content[:400].replace('\n', ' ')
            logger.info(f"    {i}. [{doc.score:.4f}] {source}")
            logger.info(f"       {preview}...")
        
        context.stage_metadata["rrf_fusion"] = {"n_results": len(context.fused_documents)}
        return context
    
    def get_config(self) -> Dict[str, Any]:
        return {"k": self.k, "top_k": self.top_k}
    
    def update_config(self, **kwargs):
        if 'k' in kwargs:
            self.k = kwargs['k']
        if 'top_k' in kwargs:
            self.top_k = kwargs['top_k']


class RerankingStage(RetrievalStage):
    """
    Cross-Encoder é‡æ’é˜¶æ®µ
    
    ä½¿ç”¨ RerankingModelManager ç®¡ç†çš„ CrossEncoder æ¨¡å‹è¿›è¡Œç²¾æ’ã€‚
    
    æ³¨æ„ï¼šå³ä½¿ enabled=Falseï¼Œé˜¶æ®µä»ç„¶æ‰§è¡Œï¼Œåªæ˜¯è·³è¿‡å®é™…é‡æ’é€»è¾‘ï¼Œ
    å°† fused_documents ä¼ é€’ç»™ä¸‹æ¸¸ã€‚è¿™ç¡®ä¿æ•°æ®æµä¸ä¼šä¸­æ–­ã€‚
    """
    
    def __init__(self):
        from config import RERANKING_ENABLED, RERANKING_MODEL, RERANKING_TOP_K, RERANKING_BATCH_SIZE
        
        self._model_manager = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.enabled = RERANKING_ENABLED
        self.model_name = RERANKING_MODEL
        self.top_k = RERANKING_TOP_K
        self.batch_size = RERANKING_BATCH_SIZE
    
    @property
    def name(self) -> str:
        return "Cross-Encoder Reranking"
    
    def is_enabled(self) -> bool:
        # å§‹ç»ˆè¿”å› Trueï¼Œç¡®ä¿æ•°æ®æµä¸ä¸­æ–­
        # å®é™…çš„é‡æ’é€»è¾‘åœ¨ execute ä¸­æ ¹æ® self.enabled åˆ¤æ–­
        return True
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        
        @timed("Cross-Encoder Reranking")
        def _do_rerank():
            documents = context.fused_documents
            
            if not self.enabled or not documents:
                return documents[:self.top_k] if documents else []
            
            model = self._get_model()
            if model is None:
                return documents[:self.top_k]
            
            try:
                pairs = [(context.original_query, doc.page_content) for doc in documents]
                scores = model.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)
                
                scored_docs = [
                    ScoredDocument(
                        document=doc.document,
                        score=float(score),
                        source="reranker",
                        metadata={**doc.metadata, "original_score": doc.score}
                    )
                    for doc, score in zip(documents, scores)
                ]
                scored_docs.sort(key=lambda x: x.score, reverse=True)
                return scored_docs[:self.top_k]
                
            except Exception as e:
                logger.error(f"Reranking failed: {e}")
                return documents[:self.top_k]
        
        context.reranked_documents = _do_rerank()
        
        # æ‰“å°é‡æ’ç»“æœ
        status = "âœ“" if self.enabled else "â­ disabled"
        logger.info(f"[Reranking] {status} Output: {len(context.reranked_documents)} documents (top 5 shown)")
        for i, doc in enumerate(context.reranked_documents[:5], 1):
            source = os.path.basename(doc.doc_metadata.get('source', 'unknown'))
            preview = doc.page_content[:400].replace('\n', ' ')
            logger.info(f"    {i}. [{doc.score:.4f}] {source}")
            logger.info(f"       {preview}...")
        
        context.stage_metadata["reranking"] = {
            "n_results": len(context.reranked_documents),
            "enabled": self.enabled
        }
        return context
    
    def _get_model(self):
        """é€šè¿‡ Manager è·å–æ¨¡å‹ï¼ˆç»Ÿä¸€ç®¡ç†ã€å¸¦ç¼“å­˜ï¼‰"""
        if self._model_manager is None:
            from managers import RerankingModelManager
            self._model_manager = RerankingModelManager()
        return self._model_manager.get_model()
    
    def get_config(self) -> Dict[str, Any]:
        return {
            "enabled": self.enabled,
            "model": self.model_name,
            "top_k": self.top_k
        }
    
    def update_config(self, **kwargs):
        if 'enabled' in kwargs:
            self.enabled = kwargs['enabled']
        if 'top_k' in kwargs:
            self.top_k = kwargs['top_k']


class ScoreTruncationStage(RetrievalStage):
    """
    æ™ºèƒ½åˆ†æ•°æˆªæ–­é˜¶æ®µ
    
    ä½¿ç”¨åŒé‡ä¿æŠ¤æœºåˆ¶è¿‡æ»¤ä½ç›¸å…³æ–‡æ¡£ï¼š
    1. ç›¸å¯¹åˆ†æ•°å·®æˆªæ–­ï¼šæ£€æµ‹åˆ†æ•°"æ–­å´–"ï¼Œåœ¨å¤§å·®è·å¤„æˆªæ–­
    2. ç»å¯¹åˆ†æ•°ä¸‹é™ï¼šè¿‡æ»¤æ‰åˆ†æ•°è¿‡ä½çš„æ–‡æ¡£
    3. ä¿åº•ç­–ç•¥ï¼šè‡³å°‘è¿”å› top-1ï¼Œå¹¶æ ‡è®° low_confidence
    """
    
    def __init__(self):
        from config import (
            SCORE_TRUNCATION_ENABLED,
            SCORE_GAP_THRESHOLD,
            SCORE_MIN_THRESHOLD
        )
        
        self.enabled = SCORE_TRUNCATION_ENABLED
        self.gap_threshold = SCORE_GAP_THRESHOLD
        self.min_threshold = SCORE_MIN_THRESHOLD
    
    @property
    def name(self) -> str:
        return "Score Truncation"
    
    def is_enabled(self) -> bool:
        return True  # å§‹ç»ˆæ‰§è¡Œï¼Œå†…éƒ¨åˆ¤æ–­æ˜¯å¦å®é™…æˆªæ–­
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        
        @timed("Score Truncation")
        def _do_truncate():
            documents = context.reranked_documents
            
            if not documents:
                return [], True  # ç©ºæ–‡æ¡£ï¼Œä½ç½®ä¿¡åº¦
            
            if not self.enabled:
                # æœªå¯ç”¨æˆªæ–­ï¼Œç›´æ¥ä¼ é€’
                return documents, False
            
            # Step 1: ç»å¯¹åˆ†æ•°è¿‡æ»¤
            filtered = [d for d in documents if d.score > self.min_threshold]
            
            # Step 2: ç›¸å¯¹åˆ†æ•°å·®æˆªæ–­ï¼ˆæ£€æµ‹æ–­å´–ï¼‰
            result = []
            for i, doc in enumerate(filtered):
                result.append(doc)
                if i < len(filtered) - 1:
                    gap = filtered[i].score - filtered[i + 1].score
                    if gap > self.gap_threshold:
                        logger.info(f"[ScoreTruncation] Gap detected: {filtered[i].score:.2f} â†’ {filtered[i+1].score:.2f} (gap={gap:.2f} > {self.gap_threshold})")
                        break
            
            # Step 3: ä¿åº•ç­–ç•¥
            if not result:
                # æ²¡æœ‰é€šè¿‡è¿‡æ»¤çš„æ–‡æ¡£ï¼Œè¿”å› top-1 + ä½ç½®ä¿¡åº¦
                logger.info(f"[ScoreTruncation] No docs passed filters, fallback to top-1 with low_confidence")
                return [documents[0]], True
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯ä½åˆ†
            top_score = result[0].score if result else 0
            low_confidence = top_score < self.min_threshold
            
            return result, low_confidence
        
        truncated, low_confidence = _do_truncate()
        context.truncated_documents = truncated
        context.low_confidence = low_confidence
        
        # æ‰“å°æˆªæ–­ç»“æœ
        original_count = len(context.reranked_documents)
        truncated_count = len(truncated)
        confidence_str = "âš  LOW CONFIDENCE" if low_confidence else "âœ“ normal"
        
        logger.info(f"[ScoreTruncation] {original_count} â†’ {truncated_count} documents ({confidence_str})")
        if truncated_count < original_count:
            logger.info(f"[ScoreTruncation] Removed {original_count - truncated_count} low-relevance documents")
        
        context.stage_metadata["score_truncation"] = {
            "original_count": original_count,
            "truncated_count": truncated_count,
            "low_confidence": low_confidence,
            "gap_threshold": self.gap_threshold,
            "min_threshold": self.min_threshold
        }
        
        return context
    
    def get_config(self) -> Dict[str, Any]:
        return {
            "enabled": self.enabled,
            "gap_threshold": self.gap_threshold,
            "min_threshold": self.min_threshold
        }
    
    def update_config(self, **kwargs):
        if 'enabled' in kwargs:
            self.enabled = kwargs['enabled']
        if 'gap_threshold' in kwargs:
            self.gap_threshold = kwargs['gap_threshold']
        if 'min_threshold' in kwargs:
            self.min_threshold = kwargs['min_threshold']


class MMRStage(RetrievalStage):
    """
    MMR å¤šæ ·æ€§åå¤„ç†é˜¶æ®µ
    
    æ³¨æ„ï¼šä½œä¸ºæµæ°´çº¿çš„æœ€åä¸€ä¸ªé˜¶æ®µï¼Œå¿…é¡»å§‹ç»ˆæ‰§è¡Œä»¥è®¾ç½® final_documentsã€‚
    mode="never" æ—¶è·³è¿‡ MMR é€»è¾‘ï¼Œç›´æ¥æˆªå–ã€‚
    """
    
    def __init__(self, embedding_function=None):
        from config import MMR_MODE, MMR_SIMILARITY_THRESHOLD, MMR_LAMBDA, MMR_FINAL_K
        
        self._embedding_function = embedding_function
        self.mode = MMR_MODE  # auto | always | never
        self.similarity_threshold = MMR_SIMILARITY_THRESHOLD
        self.lambda_mult = MMR_LAMBDA
        self.final_k = MMR_FINAL_K
    
    @property
    def name(self) -> str:
        return "MMR Post-processing"
    
    def is_enabled(self) -> bool:
        # å§‹ç»ˆè¿”å› Trueï¼Œå› ä¸ºè¿™æ˜¯æœ€åä¸€ä¸ªé˜¶æ®µï¼Œå¿…é¡»è®¾ç½® final_documents
        return True
    
    def set_embedding_function(self, fn):
        self._embedding_function = fn
    
    def execute(self, context: RetrievalContext) -> RetrievalContext:
        from managers.timing import timed
        import numpy as np
        
        @timed("MMR Post-processing")
        def _do_mmr():
            # ä½¿ç”¨æˆªæ–­åçš„æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é‡æ’åçš„æ–‡æ¡£
            documents = context.truncated_documents if context.truncated_documents else context.reranked_documents
            
            if not documents:
                return []
            
            if self.mode == "never":
                return documents[:self.final_k]
            
            # å¦‚æœæ–‡æ¡£æ•°é‡å·²ç»å¾ˆå°‘ï¼Œè·³è¿‡ MMR
            if len(documents) <= 2:
                logger.info(f"[MMR] Only {len(documents)} documents, skipping MMR")
                return documents[:self.final_k]
            
            embedding_fn = self._get_embedding_function()
            if embedding_fn is None:
                return documents[:self.final_k]
            
            should_apply = self.mode == "always"
            avg_sim = 0.0
            
            if self.mode == "auto":
                avg_sim = self._compute_avg_similarity(documents, embedding_fn)
                should_apply = avg_sim > self.similarity_threshold
                logger.info(f"[MMR] Auto-check: avg_similarity={avg_sim:.4f}, threshold={self.similarity_threshold}")
                if should_apply:
                    logger.info(f"[MMR] â†’ Similarity {avg_sim:.4f} > {self.similarity_threshold}, applying MMR")
                else:
                    logger.info(f"[MMR] â†’ Similarity {avg_sim:.4f} â‰¤ {self.similarity_threshold}, skipping MMR")
            
            if should_apply:
                return self._apply_mmr(documents, embedding_fn)
            else:
                return documents[:self.final_k]
        
        context.final_documents = _do_mmr()
        
        # æ‰“å°æœ€ç»ˆç»“æœï¼ˆè¿™äº›å°†è¾“å…¥åˆ° LLMï¼‰
        logger.info(f"[MMR] mode={self.mode} â†’ Final: {len(context.final_documents)} documents for LLM")
        logger.info("â”€" * 50)
        for i, doc in enumerate(context.final_documents, 1):
            source = os.path.basename(doc.doc_metadata.get('source', 'unknown'))
            preview = doc.page_content[:400].replace('\n', ' ')
            logger.info(f"  ğŸ“„ {i}. [{doc.score:.4f}] {source}")
            logger.info(f"     {preview}...")
        logger.info("â”€" * 50)
        
        context.stage_metadata["mmr"] = {
            "n_results": len(context.final_documents),
            "mode": self.mode
        }
        return context
    
    def _get_embedding_function(self):
        """è·å– embedding å‡½æ•°ï¼ˆç”± orchestrator æ³¨å…¥ï¼‰"""
        if self._embedding_function is None:
            logger.warning("Embedding function not set, MMR will use simple truncation")
        return self._embedding_function
    
    def _compute_avg_similarity(self, documents, embedding_fn) -> float:
        import numpy as np
        if len(documents) < 2:
            return 0.0
        try:
            embeddings = [np.array(embedding_fn(doc.page_content)) for doc in documents[:10]]
            similarities = []
            for i in range(len(embeddings)):
                for j in range(i + 1, len(embeddings)):
                    sim = np.dot(embeddings[i], embeddings[j]) / (
                        np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                    )
                    similarities.append(sim)
            return float(np.mean(similarities)) if similarities else 0.0
        except:
            return 0.0
    
    def _apply_mmr(self, documents, embedding_fn) -> List[ScoredDocument]:
        import numpy as np
        
        if len(documents) <= self.final_k:
            return documents
        
        try:
            doc_embeddings = [np.array(embedding_fn(doc.page_content)) for doc in documents]
            
            selected = [0]
            remaining = list(range(1, len(documents)))
            
            while len(selected) < self.final_k and remaining:
                best_score, best_idx = float('-inf'), None
                
                for idx in remaining:
                    relevance = documents[idx].score
                    max_sim = max(
                        np.dot(doc_embeddings[idx], doc_embeddings[sel]) / (
                            np.linalg.norm(doc_embeddings[idx]) * np.linalg.norm(doc_embeddings[sel])
                        )
                        for sel in selected
                    )
                    mmr_score = self.lambda_mult * relevance - (1 - self.lambda_mult) * max_sim
                    
                    if mmr_score > best_score:
                        best_score, best_idx = mmr_score, idx
                
                if best_idx is not None:
                    selected.append(best_idx)
                    remaining.remove(best_idx)
                else:
                    break
            
            return [
                ScoredDocument(
                    document=documents[i].document,
                    score=documents[i].score,
                    source="mmr",
                    metadata={**documents[i].metadata, "mmr_selected": True}
                )
                for i in selected
            ]
        except Exception as e:
            logger.error(f"MMR failed: {e}")
            return documents[:self.final_k]
    
    def get_config(self) -> Dict[str, Any]:
        return {
            "enabled": self.mode != "never",
            "mode": self.mode,
            "similarity_threshold": self.similarity_threshold,
            "lambda_mult": self.lambda_mult,
            "final_k": self.final_k
        }
    
    def update_config(self, **kwargs):
        if 'mode' in kwargs and kwargs['mode'] in ("auto", "always", "never"):
            self.mode = kwargs['mode']
        if 'similarity_threshold' in kwargs:
            self.similarity_threshold = kwargs['similarity_threshold']
        if 'lambda_mult' in kwargs:
            self.lambda_mult = kwargs['lambda_mult']
        if 'final_k' in kwargs:
            self.final_k = kwargs['final_k']

